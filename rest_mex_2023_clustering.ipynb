{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB7hwadSuQsN",
        "outputId": "7d92a15f-cb71-42e2-f255-8abd40897752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rest_mex_2023_clustering_task'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 14 (delta 2), reused 6 (delta 2), pack-reused 8\u001b[K\n",
            "Unpacking objects: 100% (14/14), 87.64 MiB | 6.50 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# Clone from GitHub\n",
        "!git clone https://github.com/ajhglez99/rest_mex_2023_clustering_task.git\n",
        "!unzip \"/content/rest_mex_2023_clustering_task/datasets/dataset_cleaned.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the dataset from sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# viz libs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import text_preprocess\n",
        "\n",
        "\n",
        "def tf_idf_vectorization(df):\n",
        "    # initialize the vectorizer\n",
        "    vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)\n",
        "    # fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\n",
        "    X = vectorizer.fit_transform(df['News'].apply(lambda x: np.str_(x)))\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def text_clustering(df, X):\n",
        "    # initialize kmeans with 3 centroids\n",
        "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "    # fit the model\n",
        "    kmeans.fit(X)\n",
        "    # store cluster labels in a variable\n",
        "    clusters = kmeans.labels_\n",
        "\n",
        "    # dimensional_reduction\n",
        "    # initialize PCA with 2 components\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    # pass our X to the pca and store the reduced vectors into pca_vecs\n",
        "    pca_vecs = pca.fit_transform(X.toarray())\n",
        "    # save our two dimensions into x0 and x1\n",
        "    x0 = pca_vecs[:, 0]\n",
        "    x1 = pca_vecs[:, 1]\n",
        "\n",
        "    # assign clusters and PCA vectors to columns in the original dataframe\n",
        "    df['cluster'] = clusters\n",
        "    df['x0'] = x0\n",
        "    df['x1'] = x1\n",
        "\n",
        "    # map clusters to appropriate labels\n",
        "    cluster_map = {0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\"}\n",
        "    # apply mapping\n",
        "    df['cluster'] = df['cluster'].map(cluster_map)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def vizualice(df):\n",
        "    # set image size\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    # set a title\n",
        "    plt.title(\"TF-IDF + KMeans Rest-Mex\",\n",
        "              fontdict={\"fontsize\": 18})\n",
        "    # set axes names\n",
        "    plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n",
        "    plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n",
        "    # create scatter plot with seaborn, where hue is the class used to group the data\n",
        "    sns.scatterplot(data=df, x='x0', y='x1', hue='cluster', palette=\"viridis\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv('/content/dataset_cleaned.csv')\n",
        "    # df = pd.read_csv('./datasets/dataset_translated.csv')\n",
        "\n",
        "    # df['News'] = df['News'].apply(\n",
        "    #    lambda x: text_preprocess.preprocess(x, remove_stopwords=True))\n",
        "\n",
        "    X = tf_idf_vectorization(df)\n",
        "    df_clustered = text_clustering(df, X)\n",
        "    vizualice(df_clustered)\n",
        "\n",
        "    df_clustered.to_csv('./datasets/dataset_clustered.csv', index=False)\n"
      ],
      "metadata": {
        "id": "BxXUoxjj4JBr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}